{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0ddd0839-b7c5-4cbf-a06e-e68db657dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Setup: Checking Libraries ---\n",
      "--- Dependencies Ready ---\n",
      "\n",
      "API Keys Loaded Successfully.\n",
      "Device set to use CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Clients Initialized & Ready.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import queue\n",
    "import pyaudio\n",
    "import json\n",
    "import threading\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import websocket\n",
    "from collections import deque\n",
    "\n",
    "# Dependency check and installation\n",
    "\n",
    "def is_package_installed(module_name):\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "print(\"--- Initial Setup: Checking Libraries ---\")\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    (\"google-genai\", \"google.genai\"),\n",
    "    (\"tavily-python\", \"tavily\"),\n",
    "    (\"pyaudio\", \"pyaudio\"),\n",
    "    (\"deepgram-sdk\", \"deepgram\"),\n",
    "    (\"websocket-client\", \"websocket\"),\n",
    "    (\"python-dotenv\", \"dotenv\"),\n",
    "    (\"groq\", \"groq\"),\n",
    "    (\"transformers\", \"transformers\"),       # Hugging Face\n",
    "    (\"torch\", \"torch\"),                       # PyTorch backend\n",
    "    (\"huggingface-hub\", \"huggingface_hub\")   # Hugging Face API support\n",
    "]\n",
    "\n",
    "for package_name, module_name in REQUIRED_PACKAGES:\n",
    "    if not is_package_installed(module_name):\n",
    "        print(f\"  -> Installing '{package_name}'...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"  -> Installed '{package_name}'.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"  -> Failed to install '{package_name}': {e}\")\n",
    "\n",
    "print(\"--- Dependencies Ready ---\")\n",
    "\n",
    "# Import AI Libraries\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from tavily import TavilyClient\n",
    "from groq import Groq\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load API Keys From Local Files\n",
    "\n",
    "def load_key(filename):\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "GEMINI_KEY = load_key(\"Google-Hackathon-API-Key.txt\")\n",
    "TAVILY_KEY = load_key(\"Tavily-Hackathon-API-Key.txt\")\n",
    "DEEPGRAM_KEY = load_key(\"Deepgram-Hackathon-API-Key.txt\")\n",
    "GROQ_KEY = load_key(\"Groq-Hackathon-API-Key.txt\")\n",
    "HF_KEY = load_key(\"HuggingFace-Hackathon-API-Key.txt\")  # Hugging Face\n",
    "\n",
    "if not all([GEMINI_KEY, TAVILY_KEY, DEEPGRAM_KEY, GROQ_KEY, HF_KEY]):\n",
    "    print(\"\\nSTOP: Missing API Keys. Please ensure all text files exist.\")\n",
    "else:\n",
    "    print(\"\\nAPI Keys Loaded Successfully.\")\n",
    "\n",
    "# Initialize Global Clients\n",
    "\n",
    "try:\n",
    "    gemini_client = genai.Client(api_key=GEMINI_KEY)\n",
    "    tavily_client = TavilyClient(api_key=TAVILY_KEY)\n",
    "    groq_client = Groq(api_key=GROQ_KEY)\n",
    "\n",
    "    # Hugging Face pipeline with GPU detection\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_KEY\n",
    "    device = 0 if torch.cuda.is_available() else -1  # 0 = first GPU, -1 = CPU\n",
    "    print(f\"Device set to use {'GPU' if device>=0 else 'CPU'}\")\n",
    "    hf_classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=\"facebook/bart-large-mnli\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # conversation history\n",
    "    FULL_TRANSCRIPT = deque(maxlen=50)\n",
    "\n",
    "    print(\"AI Clients Initialized & Ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Client Init Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "42c12327-4343-4884-a872-339aba7958e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Restructuring Function Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Query Restructuring Function (uses Groq)\n",
    "async def restructure_query(context_text, raw_query):\n",
    "    \"\"\"\n",
    "    Rewrites ambiguous user questions into standalone search queries using context.\n",
    "    Uses Groq's Llama model for fast query rewriting.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a query rewriting engine.\n",
    "\n",
    "Your task is to take the ambiguous user question and rewrite it into a fully\n",
    "standalone web search query by resolving all missing details using the context.\n",
    "\n",
    "Use the structured JSON object below as STRICT INPUT:\n",
    "\n",
    "{{\n",
    "  \"context\": {context_text!r},\n",
    "  \"raw_question\": {raw_query!r}\n",
    "}}\n",
    "\n",
    "MANDATORY RULES:\n",
    "1. The rewritten query MUST be fully understandable without the context.\n",
    "2. You can use \"context\" to resolve vague references. but other times the previous thing said might not at all matter\n",
    "   Example: \"the 17\" → \"the iPhone 17\" because context discusses an iPhone.\n",
    "3. Replace any pronouns, short references, or implicit subjects with explicit nouns from context.\n",
    "4. Do NOT preserve ambiguity — resolve it using if needed context.\n",
    "5. Do NOT change the meaning of the question.\n",
    "6. Output one single rewritten query.\n",
    "7. Max 370 characters.\n",
    "8. Output ONLY the rewritten query. No JSON. No quotes. No explanations.\n",
    "\n",
    "Now produce the rewritten standalone search query:\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = await asyncio.to_thread(\n",
    "            groq_client.chat.completions.create,\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        result = completion.choices[0].message.content.strip()\n",
    "        return result if result else raw_query\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Restructure Error: {e}\")\n",
    "        return raw_query\n",
    "\n",
    "print(\"Query Restructuring Function Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e2831d18-54b4-4d86-9e7f-8f5a9dfd9edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify_intent function loaded (context ignored).\n"
     ]
    }
   ],
   "source": [
    "async def classify_intent(raw_text: str, conversation_context: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Classify user input using Groq LLaMA model into:\n",
    "    - \"Factual\": question requires a fact-based answer\n",
    "    - \"Product-Opinion\": question asks about opinions on a product, service, or topic\n",
    "    - Returns None for personal/self-opinion questions (User-Opinion)\n",
    "    Prints the detected type.\n",
    "\n",
    "    NOTE: conversation_context is ignored; only the raw_text is used.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # --- Ensure it's a question ---\n",
    "        lower_text = raw_text.lower().strip()\n",
    "        question_words = [\n",
    "            \"what\", \"where\", \"when\", \"why\", \"how\", \"who\", \"which\",\n",
    "            \"do\", \"does\", \"is\", \"are\", \"can\", \"could\", \"would\", \"will\"\n",
    "        ]\n",
    "        if not (lower_text.endswith(\"?\") or any(lower_text.startswith(w + \" \") for w in question_words)):\n",
    "            return None  # ignore statements\n",
    "\n",
    "        print(\"\\n-> Agent thinking about question...\")\n",
    "\n",
    "        # --- Prompt only uses the raw question ---\n",
    "        prompt = f\"\"\"\n",
    "You are a context-aware question classifier. Classify the question into one of three categories:\n",
    "\n",
    "1. Factual - asks for factual information or knowledge.\n",
    "2. Product-Opinion - asks about opinions regarding a product, service, or topic.\n",
    "3. User-Opinion - asks about the user's personal feelings, thoughts, or state. \n",
    "   For User-Opinion questions, respond with \"None\".\n",
    "\n",
    "Rules:\n",
    "- Respond **exactly** with \"Factual\", \"Product-Opinion\", or \"None\".\n",
    "\n",
    "Examples:\n",
    "\n",
    "Factual:\n",
    "- What is the price of the iPhone 17?\n",
    "- Where can I buy a Samsung Galaxy S25?\n",
    "\n",
    "Product-Opinion:\n",
    "- How do people feel about the new iPhone 17?\n",
    "- Are users satisfied with the Xbox Series X?\n",
    "\n",
    "User-Opinion (return None):\n",
    "- How are you feeling today?\n",
    "- Do you like yourself?\n",
    "\n",
    "QUESTION: \"{raw_text}\"\n",
    "\"\"\"\n",
    "        completion = await asyncio.to_thread(\n",
    "            groq_client.chat.completions.create,\n",
    "            model=\"moonshotai/kimi-k2-instruct\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=30\n",
    "        )\n",
    "\n",
    "        result = completion.choices[0].message.content.strip().strip('\"').strip()\n",
    "\n",
    "        if result in [\"Factual\", \"Product-Opinion\"]:\n",
    "            print(f'Preparing to answer \"{result}\" type of question')\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[classify_intent] Error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"classify_intent function loaded (context ignored).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "477cc769-ff25-4c7d-ac00-18464c68819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Real-Time Sales Agent (Watcher Mode) ---\n",
      "Connected! Listening...\n",
      "[Om]: Hey, Google. What are your thoughts on the iPhone 17? \n",
      "\n",
      "-> Agent thinking about question...\n",
      "Preparing to answer \"Product-Opinion\" type of question\n",
      "-> Agent rewording the question: What are your thoughts on the iPhone 17\n",
      "-> Agent answering question...\n",
      "\n",
      "[G&T Agents]: Well, it sounds like the iPhone 17 could be the best iPhone yet, with some reviewers saying Apple finally delivered what people have been asking for. One reviewer even said its low-light camera performance is unmatched, even compared to the Galaxy S25 and Pixel 10.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main Agent Logic and Execution\n",
    "\n",
    "# --- 1. Audio Streaming Infrastructure ---\n",
    "\n",
    "class MicrophoneStream:\n",
    "    def __init__(self, rate=16000, chunk=1024):\n",
    "        self.rate = rate\n",
    "        self.chunk = chunk\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.queue = queue.Queue()\n",
    "        self.stream = None\n",
    "\n",
    "    def callback(self, in_data, frame_count, time_info, status):\n",
    "        self.queue.put(in_data)\n",
    "        return (None, pyaudio.paContinue)\n",
    "\n",
    "    def start(self):\n",
    "        self.stream = self.p.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=1,\n",
    "            rate=self.rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.chunk,\n",
    "            stream_callback=self.callback\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def stop(self):\n",
    "        if self.stream:\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "        self.p.terminate()\n",
    "\n",
    "    def get_data(self):\n",
    "        try:\n",
    "            return self.queue.get(timeout=0.1)\n",
    "        except queue.Empty:\n",
    "            return None\n",
    "\n",
    "# --- 2. Helper Tools ---\n",
    "\n",
    "def get_full_context(current_speaking_text, current_role):\n",
    "    history_list = list(FULL_TRANSCRIPT)\n",
    "    if current_speaking_text:\n",
    "        history_list.append(f\"[{current_role}]: {current_speaking_text}\")\n",
    "    \n",
    "    full_text = \"\\n\".join(history_list)\n",
    "    \n",
    "    words = full_text.split()\n",
    "    if len(words) > 300:\n",
    "        return \" \".join(words[-300:])\n",
    "    return full_text\n",
    "\n",
    "def tavily_search_tool(query: str):\n",
    "    try:\n",
    "        response = tavily_client.search(query=query, search_depth=\"basic\", max_results=2)\n",
    "        results = response.get('results', [])\n",
    "        if not results: return \"No results found.\"\n",
    "        return \"\\n\".join([f\"- {re.sub(r'\\s+', ' ', r['content']).strip()} ({r['url']})\" for r in results])\n",
    "    except: return \"Search failed.\"\n",
    "\n",
    "async def run_agent_logic(current_text, speaker_role):\n",
    "    text = re.sub(r'(?i)\\b(um|uh|so|you know)\\b', '', current_text)\n",
    "    \n",
    "    if speaker_role == \"Om\":\n",
    "        parts = re.split(r'(?i)hey[\\W_]+google', text, maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            query = parts[1].strip()\n",
    "        else: return \n",
    "    else:\n",
    "        query = text\n",
    "\n",
    "    print(flush=True)\n",
    "\n",
    "    classification = await classify_intent(query)\n",
    "    \n",
    "    if \"Personal\" in classification:\n",
    "        print(f\"   [Ignored - Personal/Statement]\", flush=True)\n",
    "        FULL_TRANSCRIPT.append(f\"[{speaker_role}]: {current_text}\")\n",
    "        return\n",
    "\n",
    "    full_context = get_full_context(current_text, speaker_role)\n",
    "    search_query = await restructure_query(full_context, query)\n",
    "    \n",
    "    print(f\"-> Agent rewording the question: {search_query}\", flush=True)\n",
    "\n",
    "    search_context = await asyncio.to_thread(tavily_search_tool, search_query)\n",
    "    \n",
    "    print(f\"-> Agent answering question...\", flush=True)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    [TRANSCRIPT CONTEXT]\n",
    "    {full_context}\n",
    "    \n",
    "    [USER QUERY]\n",
    "    {query}\n",
    "    \n",
    "    [SEARCH RESULTS]\n",
    "    {search_context}\n",
    "    \n",
    "    [INSTRUCTION]\n",
    "    Answer the user's question using the search results.\n",
    "    Keep it spoken-style, helpful, and under 2 sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = await asyncio.to_thread(\n",
    "            gemini_client.models.generate_content,\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(temperature=0.0)\n",
    "        )\n",
    "        \n",
    "        answer = response.text.strip()\n",
    "        label = \"[G&T Agents]\" if \"No results\" not in search_context else \"[G Agent]\"\n",
    "        \n",
    "        print(f\"\\n{label}: {answer}\\n\", flush=True)\n",
    "        \n",
    "        FULL_TRANSCRIPT.append(f\"[{speaker_role}]: {current_text}\")\n",
    "        FULL_TRANSCRIPT.append(f\"[AI]: {answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Error]: {e}\")\n",
    "\n",
    "# --- 4. Main Loop (Watcher) ---\n",
    "\n",
    "def start_deepgram_socket(mic_queue, loop):\n",
    "    url = \"wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate=16000&channels=1&model=nova-2&smart_format=true&interim_results=false&diarize=true\"\n",
    "    headers = {\"Authorization\": f\"Token {DEEPGRAM_KEY}\"}\n",
    "\n",
    "    state = {\n",
    "        \"current_speaker\": None,\n",
    "        \"current_buffer\": [],\n",
    "        \"last_speech_time\": time.time(),\n",
    "        \"has_processed_turn\": False\n",
    "    }\n",
    "\n",
    "    def watcher_logic():\n",
    "        while True:\n",
    "            time.sleep(0.1)\n",
    "            if not state[\"current_buffer\"]: continue\n",
    "            \n",
    "            silence = time.time() - state[\"last_speech_time\"]\n",
    "            \n",
    "            if silence > 1.5 and not state[\"has_processed_turn\"]:\n",
    "                full_text = \" \".join(state[\"current_buffer\"]).strip()\n",
    "                if not full_text: continue\n",
    "\n",
    "                clean_text = full_text.lower()\n",
    "                role = \"Om\" if state[\"current_speaker\"] == 0 else \"Customer\"\n",
    "                should_trigger = False\n",
    "\n",
    "                if state[\"current_speaker\"] == 0:\n",
    "                    if re.search(r'hey[\\W_]+google', clean_text):\n",
    "                        parts = re.split(r'hey[\\W_]+google', clean_text, maxsplit=1)\n",
    "                        if len(parts) > 1 and len(parts[1].strip()) > 2:\n",
    "                            should_trigger = True\n",
    "\n",
    "                elif state[\"current_speaker\"] == 1:\n",
    "                    should_trigger = True\n",
    "                \n",
    "                if should_trigger:\n",
    "                    asyncio.run_coroutine_threadsafe(run_agent_logic(full_text, role), loop)\n",
    "                    state[\"has_processed_turn\"] = True\n",
    "                    if state[\"current_speaker\"] == 0:\n",
    "                        state[\"current_buffer\"] = []\n",
    "                \n",
    "                elif state[\"current_speaker\"] == 1:\n",
    "                    state[\"has_processed_turn\"] = True\n",
    "\n",
    "    threading.Thread(target=watcher_logic, daemon=True).start()\n",
    "\n",
    "    def on_message(ws, message):\n",
    "        try:\n",
    "            data = json.loads(message)\n",
    "            if 'channel' in data:\n",
    "                alts = data['channel']['alternatives']\n",
    "                if alts:\n",
    "                    transcript = alts[0]['transcript'].strip()\n",
    "                    if transcript:\n",
    "                        sid = 0\n",
    "                        if 'words' in alts[0]:\n",
    "                            words = alts[0]['words']\n",
    "                            if words: sid = words[-1]['speaker']\n",
    "\n",
    "                        if state[\"current_speaker\"] is not None and state[\"current_speaker\"] != sid:\n",
    "                            print()\n",
    "                            print(f\"[{'Om' if sid == 0 else 'Customer'}]: \", end=\"\", flush=True)\n",
    "                            \n",
    "                            if state[\"current_buffer\"]:\n",
    "                                old_role = \"Om\" if state[\"current_speaker\"]==0 else \"Customer\"\n",
    "                                FULL_TRANSCRIPT.append(f\"[{old_role}]: {' '.join(state['current_buffer'])}\")\n",
    "                            \n",
    "                            state[\"current_buffer\"] = [] \n",
    "                            state[\"has_processed_turn\"] = False\n",
    "                        \n",
    "                        elif state[\"current_speaker\"] is None:\n",
    "                            print(f\"[{'Om' if sid == 0 else 'Customer'}]: \", end=\"\", flush=True)\n",
    "\n",
    "                        print(f\"{transcript} \", end=\"\", flush=True)\n",
    "\n",
    "                        state[\"current_speaker\"] = sid\n",
    "                        state[\"current_buffer\"].append(transcript)\n",
    "                        state[\"last_speech_time\"] = time.time()\n",
    "                        \n",
    "                        if sid == 1: state[\"has_processed_turn\"] = False\n",
    "\n",
    "        except Exception: pass\n",
    "\n",
    "    ws = websocket.WebSocketApp(url, header=headers, on_open=lambda ws: print(\"Connected! Listening...\", flush=True), on_message=on_message)\n",
    "    \n",
    "    def send_audio():\n",
    "        while True:\n",
    "            try:\n",
    "                data = mic_queue.get(timeout=1)\n",
    "                ws.send(data, opcode=websocket.ABNF.OPCODE_BINARY)\n",
    "            except: continue\n",
    "    \n",
    "    threading.Thread(target=send_audio, daemon=True).start()\n",
    "    ws.run_forever()\n",
    "\n",
    "# --- 5. Execution ---\n",
    "\n",
    "async def main_loop():\n",
    "    print(\"--- Real-Time Sales Agent (Watcher Mode) ---\")\n",
    "    mic = MicrophoneStream()\n",
    "    mic.start()\n",
    "    loop = asyncio.get_running_loop()\n",
    "    threading.Thread(target=start_deepgram_socket, args=(mic.queue, loop), daemon=True).start()\n",
    "    try:\n",
    "        while True: await asyncio.sleep(1)\n",
    "    except: pass\n",
    "    finally: mic.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        if \"ipykernel\" in sys.modules: await main_loop()\n",
    "        else: asyncio.run(main_loop())\n",
    "    except: print(\"\\nStopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6841c81a-09ae-40c5-a68a-663ce6c4379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True] all correct\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True] all correct\n",
      "[False, True, False, False, True, False, True, False, False, True, False, False, False, True, False]\n",
      "[False, True, True, False, True, True, True, False, False, True, False, True, False, True, False]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True] all correct\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[129]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    141\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Run the test synchronously\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[43mtest_models_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[129]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mtest_models_sync\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     82\u001b[39m             prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[33mYou are a context-aware question classifier. Classify the question into one of three categories:\u001b[39m\n\u001b[32m     84\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \n\u001b[32m    111\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    112\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m                 completion = \u001b[43mgroq_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m    118\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m                 result = completion.choices[\u001b[32m0\u001b[39m].message.content.strip().strip(\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m).strip()\n\u001b[32m    120\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mFactual\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mProduct-Opinion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mUser-Opinion\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:464\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    245\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    246\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    303\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    304\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    307\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    462\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\groq\\_base_client.py:980\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    978\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    986\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# NOT PART OF MAIN RUNNING CODE EVERYTHING BELOW IS JUST FOR TESTING \n",
    "# NOT PART OF MAIN RUNNING CODE EVERYTHING BELOW IS JUST FOR TESTING \n",
    "# NOT PART OF MAIN RUNNING CODE EVERYTHING BELOW IS JUST FOR TESTING \n",
    "# NOT PART OF MAIN RUNNING CODE EVERYTHING BELOW IS JUST FOR TESTING \n",
    "\n",
    "#Testing which models work accurately for understanding question type\n",
    "questions = [\n",
    "    # --- User-Opinion (should return None) ---\n",
    "    \"How do you feel?\",\n",
    "    \"Are you happy today?\",\n",
    "    \"Do you like yourself?\",\n",
    "    \"How are you doing right now?\",\n",
    "    \"What do you think about your progress?\",\n",
    "\n",
    "    # --- Product-Opinion ---\n",
    "    \"How do you feel about the new iPhone 17?\",\n",
    "    \"Do people like the battery life on the Pixel 9?\",\n",
    "    \"What do customers think about the Tesla Model Y?\",\n",
    "    \"Is the camera quality on the Samsung Galaxy S25 good?\",\n",
    "    \"Do people enjoy using the Xbox Series X?\",\n",
    "\n",
    "    # --- Factual ---\n",
    "    \"What is the price of the new iPhone 17?\",\n",
    "    \"Where can I buy a Samsung Galaxy S25?\",\n",
    "    \"When will the new Tesla Model 3 release?\",\n",
    "    \"How many pixels does the iPhone 17 camera have?\",\n",
    "    \"What is the screen size of the Samsung Galaxy Tab S9?\"\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"How do you feel about the new iPhone 17?\",   # Product-Opinion\n",
    "    \"How do you feel?\",                           # User-Opinion\n",
    "    \"What is the screen size of the Samsung Galaxy Tab S9?\",  # Factual\n",
    "    \"Do people like the battery life on the Pixel 9?\",        # Product-Opinion\n",
    "    \"Do you like yourself?\",                       # User-Opinion\n",
    "    \"What is the price of the new iPhone 17?\",     # Factual\n",
    "    \"Are you happy today?\",                        # User-Opinion\n",
    "    \"What do customers think about the Tesla Model Y?\",       # Product-Opinion\n",
    "    \"When will the new Tesla Model 3 release?\",    # Factual\n",
    "    \"How are you doing right now?\",                # User-Opinion\n",
    "    \"Do people enjoy using the Xbox Series X?\",    # Product-Opinion\n",
    "    \"Where can I buy a Samsung Galaxy S25?\",      # Factual\n",
    "    \"How many pixels does the iPhone 17 camera have?\", # Factual\n",
    "    \"What do you think about your progress?\",     # User-Opinion\n",
    "    \"Is the camera quality on the Samsung Galaxy S25 good?\"  # Product-Opinion\n",
    "]\n",
    "\n",
    "expected = [\n",
    "    \"Product-Opinion\", None, \"Factual\", \"Product-Opinion\", None,\n",
    "    \"Factual\", None, \"Product-Opinion\", \"Factual\", None,\n",
    "    \"Product-Opinion\", \"Factual\", \"Factual\", None, \"Product-Opinion\"\n",
    "]\n",
    "\n",
    "models_to_test = [\n",
    "    #\"allam-2-7b\",\n",
    "    #\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    #\"meta-llama/llama-guard-4-12b\",\n",
    "    #\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    \"moonshotai/kimi-k2-instruct-0905\",\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    #\"openai/gpt-oss-safeguard-20b\",\n",
    "    #\"qwen/qwen3-32b\",\n",
    "    #\"meta-llama/llama-prompt-guard-2-22m\",\n",
    "    #\"llama-3.3-70b-versatile\",\n",
    "    #\"groq/compound-mini\",\n",
    "    #\"meta-llama/llama-prompt-guard-2-86m\",\n",
    "    #\"openai/gpt-oss-120b\",\n",
    "    \"moonshotai/kimi-k2-instruct\",\n",
    "    \"groq/compound\",\n",
    "    \"openai/gpt-oss-20b\"\n",
    "]\n",
    "\n",
    "\n",
    "good_models = []\n",
    "\n",
    "def test_models_sync():\n",
    "    for model_name in models_to_test:\n",
    "        results = []\n",
    "        all_correct = True\n",
    "\n",
    "        for q, expected_label in zip(questions, expected):\n",
    "            prompt = f\"\"\"\n",
    "You are a context-aware question classifier. Classify the question into one of three categories:\n",
    "\n",
    "1. Factual - asks for factual information or knowledge.\n",
    "2. Product-Opinion - asks about opinions regarding a product, service, or topic.\n",
    "3. User-Opinion - asks about the user's personal feelings, thoughts, or state. \n",
    "   For User-Opinion questions, respond with \"None\".\n",
    "\n",
    "Rules:\n",
    "- Use context if needed to disambiguate.\n",
    "- Respond **exactly** with \"Factual\", \"Product-Opinion\", or \"None\".\n",
    "\n",
    "Examples:\n",
    "\n",
    "Factual:\n",
    "- What is the price of the iPhone 17?\n",
    "- Where can I buy a Samsung Galaxy S25?\n",
    "\n",
    "Product-Opinion:\n",
    "- How do people feel about the new iPhone 17?\n",
    "- Are users satisfied with the Xbox Series X?\n",
    "\n",
    "User-Opinion (return None):\n",
    "- How are you feeling today?\n",
    "- Do you like yourself?\n",
    "\n",
    "QUESTION: \"{q}\"\n",
    "Respond with only one label.\n",
    "\n",
    "\"\"\"\n",
    "            try:\n",
    "                completion = groq_client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.0,\n",
    "                    max_tokens=50\n",
    "                )\n",
    "                result = completion.choices[0].message.content.strip().strip('\"').strip()\n",
    "                if result not in [\"Factual\", \"Product-Opinion\", \"User-Opinion\"]:\n",
    "                    result = None\n",
    "\n",
    "                results.append(result == expected_label)\n",
    "\n",
    "                if result != expected_label:\n",
    "                    all_correct = False\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{q}] Error: {e}\")\n",
    "                results.append(False)\n",
    "                all_correct = False\n",
    "\n",
    "        if all_correct:\n",
    "            good_models.append(model_name)\n",
    "            print(f\"{results} all correct\")\n",
    "        else:\n",
    "            print(results)\n",
    "\n",
    "    print(\"\\n=== Summary: Models that got all correct ===\")\n",
    "    for m in good_models:\n",
    "        print(f\"- {m}\")\n",
    "\n",
    "# Run the test synchronously\n",
    "test_models_sync()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42350fba-9d9c-47c0-ac87-11ab601314fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
